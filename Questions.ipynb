{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fbfd5c-eb80-4dc7-b8d4-c773449f30f9",
   "metadata": {},
   "source": [
    "# 1. What steps are missing to industrialize such solution further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5d527-2d89-4c5a-9a03-2c721d238fd4",
   "metadata": {},
   "source": [
    "The current solution is a manually-run notebook, which is a great start. To industrialize it—making it a robust, automated, and reliable production asset—we are missing several key steps:\n",
    "\n",
    "- Orchestration & Scheduling:\n",
    "\n",
    "Problem: We have to run the notebook by clicking \"Run all.\"\n",
    "\n",
    "Solution: We need to schedule this pipeline. The simplest native way is to create a Databricks Workflow. This would run our notebook on a schedule (e.g., hourly or daily). More advanced ecosystems might use an external orchestrator like Airflow or Dagster.\n",
    "\n",
    "- Incremental Processing:\n",
    "\n",
    "Problem: Our pipeline uses mode(\"overwrite\") on all tables. This is fine for item.csv (a small dimension table), but it is disastrous for event.csv (a massive fact table). We are re-processing the entire history every time.\n",
    "\n",
    "Solution: The pipeline must be made incremental.\n",
    "\n",
    "- Ingestion: The best practice is to use Databricks Auto Loader instead of our spark.read.csv() script. Auto Loader can efficiently and incrementally pick up only new files that land in the S3 bucket.\n",
    "\n",
    "Transformations: For the Silver and Gold layers, we would stop using mode(\"overwrite\") and switch to Delta Lake's MERGE INTO (UPSERT) commands. This would allow us to only process new or updated event data from the Bronze layer.\n",
    "\n",
    "- Data Quality Testing:\n",
    "\n",
    "Problem: We assume the data is good. What if a new event.csv file has no rows, or the item_id is NULL for every row? Our pipeline would run, and our Gold datamart would be silently corrupted.\n",
    "\n",
    "Solution: We must add Data Quality Tests. In Spark, this can be done with custom assertions (e.g., assert df.count() > 0) or libraries like Deequ. This is a major upside of dbt, which we'll discuss below.\n",
    "\n",
    "- Monitoring & Alerting:\n",
    "\n",
    "Problem: If the pipeline fails at 3 AM, how do we know? If it succeeds, how do we know how many rows it processed?\n",
    "\n",
    "Solution: We need to add Observability.\n",
    "\n",
    "Alerting: Databricks Workflows can be configured to send an email or Slack/Teams notification on failure.\n",
    "\n",
    "Monitoring: We should log key metrics (e.g., \"rows processed,\" \"rows filtered\") to a dashboard.\n",
    "\n",
    "- CI/CD & DevOps:\n",
    "\n",
    "Problem: Our code lives only in a notebook. There is no version control, no code review, and no dev/prod separation.\n",
    "\n",
    "Solution: The notebook should be integrated with Databricks Repos (which connects to GitHub/GitLab). We would create a proper CI/CD pipeline (e.g., GitHub Actions) to automatically test and deploy code changes from a dev branch to the main/prod branch.\n",
    "\n",
    "- Parameterization:\n",
    "\n",
    "Problem: All our paths and names (like CATALOG_NAME = \"workspace\") are hardcoded.\n",
    "\n",
    "Solution: The notebook should use Widgets (dbutils.widgets.text(...)). This turns the notebook into a re-usable function where the catalog name, input paths, and output databases can be passed in as parameters by the Databricks Workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adbef31-177c-4fd9-a444-58402ac1ab83",
   "metadata": {},
   "source": [
    "# 2. If the solution was implemented in dbt-core, how would the overall architecture change? Would there be another cloud resources needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892286e-dbe8-4b8f-ae01-8256b80d18ad",
   "metadata": {},
   "source": [
    "dbt is a tool for the T (Transform) in an ELT pipeline. It does not do the E (Extract) or L (Load).\n",
    "\n",
    "current notebook does ELT (Extracts from S3, Loads to Bronze, Transforms to Silver/Gold).\n",
    "\n",
    "If we used dbt-core, the architecture would split:\n",
    "\n",
    "- Ingestion (E+L) - Still Python/Spark:\n",
    "We would still need a process to get the data from the S3 URL into our Bronze layer. Our COMMAND 1 to COMMAND 5 (the ingestion and Layer 1 load) would remain a separate Python script, likely orchestrated by a Databricks Workflow. (Or, even better, this step would be replaced by a Databricks Auto Loader).\n",
    "\n",
    "- Transformation (T) - dbt-core:\n",
    "All the logic from LAYER 2: SILVER and LAYER 3: GOLD would be removed from the Spark notebook. This logic would be rewritten as .sql files in a dbt project.\n",
    "\n",
    "silver_item.sql\n",
    "silver_event.sql\n",
    "gold_top_item.sql\n",
    "\n",
    "dbt would connect directly to our Databricks SQL Warehouse (like your starter-warehouse) and run this SQL to transform data from Bronze to Silver, and from Silver to Gold.\n",
    "\n",
    "New Cloud Resources Needed:\n",
    "\n",
    "Yes. dbt-core is a command-line tool. Something needs to run it. The new resources would be:\n",
    "\n",
    "- An Orchestration/Execution Environment: We need a machine to run dbt run on a schedule. This could be:\n",
    "- A CI/CD pipeline (like GitHub Actions) running on a schedule.\n",
    "- A Docker container (e.g., on AWS Fargate or Kubernetes).\n",
    "- A traditional virtual machine (e.g., EC2) with a cron job.\n",
    "- A task in an orchestrator like Airflow.\n",
    "- Databricks SQL Warehouse: dbt works best when connecting to a SQL Warehouse, not an all-purpose cluster. We would use this (like current starter-warehouse) for the dbt transformations, which is what it's optimized for.\n",
    "- Git Repository: a dbt project is a Git repository. This becomes a hard requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da1625-8c45-4501-8f52-774b3d21dc32",
   "metadata": {},
   "source": [
    "# 3. What would implementation in dbt-core bring to the project? What would be the upsides and downsides?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6313749-d3da-478c-b15f-34b4bb39f3ed",
   "metadata": {},
   "source": [
    "#### 3.A. Upsides (Why dbt is so popular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0dabec-e10d-478f-8646-183f2fd88050",
   "metadata": {},
   "source": [
    "Built-in Testing: This is dbt's superpower. We could add a simple .yml file to test our data for free. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b83881-a9a5-4666-aa8b-38eb2e884df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models:\n",
    "  - name: silver_event\n",
    "    columns:\n",
    "      - name: event_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: item_id\n",
    "        tests:\n",
    "          - relationships:\n",
    "              to: ref('silver_item')\n",
    "              field: item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bc0f8-9af1-4bff-a130-7c614d352441",
   "metadata": {},
   "source": [
    "Running dbt test would automatically check that all event_ids are unique and that no item_ids are \"orphaned.\" This solves our Data Quality problem.\n",
    "\n",
    "- Automatic Lineage & Documentation:\n",
    "\n",
    "dbt automatically generates a full dependency graph (DAG) of models. When dbt runs, docs generate, it builds a website showing exactly how the  gold_top_item table is built from silver_event and silver_item, which are built from the Bronze sources. This is invaluable for debugging and onboarding.\n",
    "\n",
    "- SQL-First / Accessibility:\n",
    "\n",
    "The transformation logic moves from the (complex, developer-focused) PySpark DataFrame API to standard SQL files. This makes the project instantly accessible to data analysts, who can now contribute to the pipeline without needing to learn Python or Spark.\n",
    "\n",
    "- Modularity & Reusability:\n",
    "\n",
    "dbt handles all dependencies. Instead of writing spark.table(\"workspace.silver_db.event\"), you just write {{ ref('silver_event') }}. dbt automatically figures out the correct order to run all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf4d50a-9701-4ea8-8c0e-be752de4d060",
   "metadata": {},
   "source": [
    "#### 3.B. Downsides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25e5fb-fad2-4bff-b6e7-b699643566d4",
   "metadata": {},
   "source": [
    "- Ingestion is not included\n",
    "\n",
    "As mentioned, dbt does not solve our ingestion problem. We still need a separate, non-dbt process to get the data from S3 to Bronze. This adds a moving part.\n",
    "\n",
    "- Limited to SQL (Mostly)\n",
    "\n",
    "dbt thrives on SQL. The complex JSON parsing (which is solved with multiLine and escape) would need to be translated to Databricks from_json SQL function. This is 100% possible, but any more complex Python-based parsing (like using a UDF) becomes much harder to integrate.\n",
    "\n",
    "- Added Complexity:\n",
    "\n",
    "We now have two systems to manage: the Python Ingestion script and the dbt Transformation project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19639700-139e-4630-bb6a-d1e166fcb45a",
   "metadata": {},
   "source": [
    "# 4. Please estimate the effort you’d requested to implement the solution in dbt-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb7603-dd7b-4cf4-a83f-82ee6db75cf3",
   "metadata": {},
   "source": [
    "Total Effort Estimate: ~2 Days (12-16 hours)\n",
    "\n",
    "- Project Setup (2 hours):\n",
    "\n",
    "dbt init, configure profiles.yml to connect to the Databricks SQL Warehouse, set up dbt_project.yml, and create the Git repo.\n",
    "\n",
    "- Bronze Layer (1 hour):\n",
    "\n",
    "Define the bronze_db tables as sources in a .yml file.\n",
    "\n",
    "- Silver Layer (4-6 hours):\n",
    "\n",
    "Create models/silver/silver_item.sql. (Straightforward SELECT with CAST and COALESCE).\n",
    "\n",
    "Create models/silver/silver_event.sql. (This is the trickiest part: using from_json in SQL to flatten the payload, casting, and setting the partition_by config).\n",
    "\n",
    "Add data tests (unique, not_null, relationships) for the key columns in a .yml file.\n",
    "\n",
    "- Gold Layer (3-4 hours):\n",
    "\n",
    "Create models/gold/gold_top_item.sql. This is very clean in dbt, using ref('silver_event') and ref('silver_item') in CTEs. The RANK() and ROW_NUMBER() functions are native SQL.\n",
    "\n",
    "- Documentation & Orchestration (2-3 hours):\n",
    "\n",
    "Add descriptions to all models/columns in the .yml files.\n",
    "\n",
    "Run dbt docs generate.\n",
    "\n",
    "Set up a basic GitHub Actions workflow to run dbt build on a schedule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
